# -*- coding: utf-8 -*-
"""working_Microg_YoloV7_model V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egxqeORZTkVPWav0EDOOWCi6SKae6R-B

#Train and Test YOLOV7
"""

!nvidia-smi

!pip install numpy==1.25.2

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/WongKinYiu/yolov7
# %cd yolov7
!pip install -r requirements.txt

!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="j19bmHHOB6TxRaOzR8X9")
project = rf.workspace("csi-microg").project("microg-2")
version = project.version(1)
dataset = version.download("yolov7")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/yolov7
!wget "https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt"

# Commented out IPython magic to ensure Python compatibility.
# #modify model parameters
# 
# %%writefile cfg/training/yolov7.yaml
# 
# # parameters
# nc: 80  # number of classes
# depth_multiple: 1.0  # model depth multiple
# width_multiple: 1.0  # layer channel multiple
# 
# # anchors
# anchors:
#   - [12,16, 19,36, 40,28]  # P3/8
#   - [36,75, 76,55, 72,146]  # P4/16
#   - [142,110, 192,243, 459,401]  # P5/32
# 
# # yolov7 backbone
# backbone:
#   # [from, number, module, args]
#   [[-1, 1, Conv, [32, 3, 1]],  # 0
# 
#    [-1, 1, Conv, [64, 3, 2]],  # 1-P1/2
#    [-1, 1, Conv, [64, 3, 1]],
# 
#    [-1, 1, Conv, [128, 3, 2]],  # 3-P2/4
#    [-1, 1, Conv, [64, 1, 1]],
#    [-2, 1, Conv, [64, 1, 1]],
#    [-1, 1, Conv, [64, 3, 1]],
#    [-1, 1, Conv, [64, 3, 1]],
#    [-1, 1, Conv, [64, 3, 1]],
#    [-1, 1, Conv, [64, 3, 1]],
#    [[-1, -3, -5, -6], 1, Concat, [1]],
#    [-1, 1, Conv, [256, 1, 1]],  # 11
# 
#    [-1, 1, MP, []],
#    [-1, 1, Conv, [128, 1, 1]],
#    [-3, 1, Conv, [128, 1, 1]],
#    [-1, 1, Conv, [128, 3, 2]],
#    [[-1, -3], 1, Concat, [1]],  # 16-P3/8
#    [-1, 1, Conv, [128, 1, 1]],
#    [-2, 1, Conv, [128, 1, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [[-1, -3, -5, -6], 1, Concat, [1]],
#    [-1, 1, Conv, [512, 1, 1]],  # 24
# 
#    [-1, 1, MP, []],
#    [-1, 1, Conv, [256, 1, 1]],
#    [-3, 1, Conv, [256, 1, 1]],
#    [-1, 1, Conv, [256, 3, 2]],
#    [[-1, -3], 1, Concat, [1]],  # 29-P4/16
#    [-1, 1, Conv, [256, 1, 1]],
#    [-2, 1, Conv, [256, 1, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [[-1, -3, -5, -6], 1, Concat, [1]],
#    [-1, 1, Conv, [1024, 1, 1]],  # 37
# 
#    [-1, 1, MP, []],
#    [-1, 1, Conv, [512, 1, 1]],
#    [-3, 1, Conv, [512, 1, 1]],
#    [-1, 1, Conv, [512, 3, 2]],
#    [[-1, -3], 1, Concat, [1]],  # 42-P5/32
#    [-1, 1, Conv, [256, 1, 1]],
#    [-2, 1, Conv, [256, 1, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [[-1, -3, -5, -6], 1, Concat, [1]],
#    [-1, 1, Conv, [1024, 1, 1]],  # 50
#   ]
# 
# # yolov7 head
# head:
#   [[-1, 1, SPPCSPC, [512]], # 51
# 
#    [-1, 1, Conv, [256, 1, 1]],
#    [-1, 1, nn.Upsample, [None, 2, 'nearest']],
#    [37, 1, Conv, [256, 1, 1]], # route backbone P4
#    [[-1, -2], 1, Concat, [1]],
# 
#    [-1, 1, Conv, [256, 1, 1]],
#    [-2, 1, Conv, [256, 1, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [[-1, -2, -3, -4, -5, -6], 1, Concat, [1]],
#    [-1, 1, Conv, [256, 1, 1]], # 63
# 
#    [-1, 1, Conv, [128, 1, 1]],
#    [-1, 1, nn.Upsample, [None, 2, 'nearest']],
#    [24, 1, Conv, [128, 1, 1]], # route backbone P3
#    [[-1, -2], 1, Concat, [1]],
# 
#    [-1, 1, Conv, [128, 1, 1]],
#    [-2, 1, Conv, [128, 1, 1]],
#    [-1, 1, Conv, [64, 3, 1]],
#    [-1, 1, Conv, [64, 3, 1]],
#    [-1, 1, Conv, [64, 3, 1]],
#    [-1, 1, Conv, [64, 3, 1]],
#    [[-1, -2, -3, -4, -5, -6], 1, Concat, [1]],
#    [-1, 1, Conv, [128, 1, 1]], # 75
# 
#    [-1, 1, MP, []],
#    [-1, 1, Conv, [128, 1, 1]],
#    [-3, 1, Conv, [128, 1, 1]],
#    [-1, 1, Conv, [128, 3, 2]],
#    [[-1, -3, 63], 1, Concat, [1]],
# 
#    [-1, 1, Conv, [256, 1, 1]],
#    [-2, 1, Conv, [256, 1, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [-1, 1, Conv, [128, 3, 1]],
#    [[-1, -2, -3, -4, -5, -6], 1, Concat, [1]],
#    [-1, 1, Conv, [256, 1, 1]], # 88
# 
#    [-1, 1, MP, []],
#    [-1, 1, Conv, [256, 1, 1]],
#    [-3, 1, Conv, [256, 1, 1]],
#    [-1, 1, Conv, [256, 3, 2]],
#    [[-1, -3, 51], 1, Concat, [1]],
# 
#    [-1, 1, Conv, [512, 1, 1]],
#    [-2, 1, Conv, [512, 1, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [-1, 1, Conv, [256, 3, 1]],
#    [[-1, -2, -3, -4, -5, -6], 1, Concat, [1]],
#    [-1, 1, Conv, [512, 1, 1]], # 101
# 
#    [75, 1, RepConv, [256, 3, 1]],
#    [88, 1, RepConv, [512, 3, 1]],
#    [101, 1, RepConv, [1024, 3, 1]],
# 
#    [[102,103,104], 1, IDetect, [nc, anchors]],   # Detect(P3, P4, P5)
#   ]

# Commented out IPython magic to ensure Python compatibility.
#added adam optimizer, evolving hyperparameters per run
#100 epochs brings accuracy to 95-100% for each class

# %cd /content/yolov7/
!python train.py --batch 16 --cfg cfg/training/yolov7.yaml --epochs 50 --evolve --adam  --data microg-2-1/data.yaml --weights 'yolov7.pt' --device 0

!python detect.py --weights runs/train/exp/weights/best.pt --conf 0.5 --source microg-2-1/test/images

#display inference on ALL test images
#display inference on ALL test images

import glob
from IPython.display import Image, display

i = 0
limit = 10000 # max images to print
for imageName in glob.glob('/content/yolov7/runs/detect/exp/*.jpg'): #assuming JPG
    if i < limit:
      display(Image(filename=imageName))
      print("\n")
    i = i + 1

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir runs

from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)

#save the weights so you can load the model later
#model.load_state_dict(torch.load(path))

from google.colab import files
files.download("/content/yolov7/runs/train/exp/weights/best.pt")

# Copy best.pt to Google Drive

from google.colab import drive
drive.mount('/content/drive')

import shutil


local_best_pt_path = '/content/yolov7/runs/train/exp/weights/best.pt'
drive_destination_path = '/content/drive/My Drive/2023/24 Micro-g NExT/Micro-g YOLO Model Workspace/results'

shutil.copy(local_best_pt_path, drive_destination_path)
drive.flush_and_unmount()

"""#Accessing trained model to run on life camera feed"""

#Once the model has been trained, here's how to access it later

from models.yolov7 import YOLOv7
import torch

# load model frameworrk
model = YOLOv7("cfg/training/yolov7.yaml")

# Load the trained weights from best.pt. only for testing, at the competition, best.pt will be in the same directory
weights_path = '/content/drive/My Drive/2023/24 Micro-g NExT/Micro-g YOLO Model Workspace/results/best.pt'
model.load_state_dict(torch.load(weights_path))

model.eval()

# Access the computer camera feed -> access Jetson Nano later
cap = cv2.VideoCapture(0)

output_dir = 'runs/train/exp/live_feed'
os.makedirs(output_dir, exist_ok=True)

#reads all of the annotated images from the first captured frame and save it in a local directory.
#The GUI should then read this directory and render the images based on precendence (read the class name(s) from file path).

# Read only first frame
ret, frame = cap.read()

# Perform object detection on the first frame
detections = model(frame)

life_ring_count = 0
lpu_count = 0
life_raft_count = 0
orion_count = 0

bbox_color = (255,255,255)
img_no = 0;

for detection in detections:
    class_label = detection['class']
    if class_label == 'life-ring':
        life_ring_count++
        bbox_text = f"{class_label} {life_ring_count}"
    elif class_label == 'LPU':
        lpu_count++
        bbox_text = f"{class_label} {lpu_count}"
    elif class_label == 'life-raft':
        life_raft_count++
        bbox_text = f"{class_label} {life_raft_count}"
    elif class_label == 'orion':
        orion_count++
        bbox_text = f"{class_label} {orion_count}"

    else:
        continue

    #draw boundeing box around the object of interest with a class label
    x, y, w, h = detection['bbox']
    x1, y1, x2, y2 = int(x - w / 2), int(y - h / 2), int(x + w / 2), int(y + h / 2)

    cv2.rectangle(frame, (x1, y1), (x2, y2), bbox_color, 2)
    cv2.putText(frame, bbox_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, bbox_color, 2)

    file_name = f"annotated_frame_{class_label}_{img_no}.jpg"
    file_path = os.path.join(output_dir, file_name)
    cv2.imwrite(file_path, frame)


cap.release()
cv2.destroyAllWindows()